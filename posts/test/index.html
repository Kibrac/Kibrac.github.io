<!DOCTYPE html>
<html lang="zh"
  dir="ltr">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="https://kibrac.github.io//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://kibrac.github.io//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://kibrac.github.io//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="https://kibrac.github.io//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://kibrac.github.io//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    Test | Kibrac
    
</title>

<link rel="canonical" href="https://kibrac.github.io/posts/test/"/>












<link rel="stylesheet" href="/assets/combined.min.a6824bbee0d90d5af09fed9b70395ce7076b615e315037455d903314e96ef91b.css" media="all">







  </head>

  

  
  
  

  <body class="auto">

    <div class="content">
      <header>
        

<div class="header">

    

</div>

      </header>

      <main class="main">
        





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/test/">Test</a>
</div>



<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Test</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-09-09T17:28:19&#43;08:00">September 9, 2024</time>
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <h2 id="11-深度学习介绍">1.1 深度学习介绍</h2>
<h3 id="111-区别">1.1.1 区别</h3>
<p><strong>深度学习和机器学习的区别</strong>：深度学习并不需要过多的特征处理。</p>
<h4 id="1111-特征提取方面">1.1.1.1 特征提取方面</h4>
<ul>
<li>机器学习的特征工程步骤是要靠手动完成的，而且需要大量领域专业知识</li>
<li>深度学习通常由多个层构成，它们通常将更简单的模型组合在一起，通过将数据从一层传递到另一层来构建更复杂的模型。通过大量数据的训练自动得到模型，不需要人工设计特征提取环节。</li>
</ul>
<blockquote>
<p>深度学习算法试图从数据中学习高级功能，这是深度学习的一个非常独特的功能。因此，减少了为了每个问题开发新特征提取器的任务。**适合用在难提取特征的图像、语音、自然语言领域。</p>
</blockquote>
<h3 id="112-深度学习的应用场景">1.1.2 深度学习的应用场景</h3>
<ul>
<li>图像识别</li>
<li>自然语言处理</li>
<li>语音技术</li>
<li>机器学习效果不行的这些场景，特征需要复杂处理的地方</li>
</ul>
<h3 id="113-深度学习代表算法---神经网络">1.1.3 深度学习代表算法 - 神经网络</h3>
<h4 id="1131-神经网络">1.1.3.1 神经网络</h4>
<p><strong>人工神经网络（Artificial Neural Network，简写为 ANN），也简称为神经网络（NN）</strong>，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）结构和功能的<strong>计算模型</strong>。经典的神经网络结构包含三个层次的神经网络，分别是<strong>输入层、隐藏层和输出层</strong>。</p>
<p><em>图中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出，输出层的神经元只是输入。</em></p>
<ul>
<li><strong>神经网络的特点</strong>
<ul>
<li>每个连接都有权值，同一层神经元之间没有连接</li>
<li>神经元当中会包含有<strong>激活函数</strong></li>
<li>最后的输出结果对应的层也称之为<strong>全连接层</strong></li>
</ul>
</li>
</ul>
<h3 id="114-为什么深度学习现在效果非常好">1.1.4 为什么深度学习现在效果非常好</h3>
<ul>
<li>数据</li>
<li>计算
<ul>
<li>GPU、TPU</li>
</ul>
</li>
<li>算法
<ul>
<li>一些创新，如 [[ReLU 激活函数]]</li>
</ul>
</li>
</ul>
<h2 id="12-神经网络基础">1.2 神经网络基础</h2>
<h3 id="121-logistic-回归">1.2.1 Logistic 回归</h3>
<h4 id="1211-logistic-回归">1.2.1.1 Logistic 回归</h4>
<p>逻辑回归是一个主要用于二分分类的算法，那么逻辑回归是给定一个 $x$，输出一个该样本属于 $1$ 对应类别的预测概率 $\hat{y}=P(y=1|x)$。</p>
<p>Logistic 回归中使用的参数如下：</p>
<ul>
<li><strong>输入的特征向量</strong>： $x\in R^{n_{x}}$ , $x$ 是一个 $n_{x}$ 维的特征数量，用于训练的标签： $y\in0,1$</li>
<li><strong>参数</strong>：权重： $w\in R^{n_{x}}$ ，偏置： $b\in R$</li>
<li><strong>输出预测结果</strong>： $\hat{y}=\sigma(w^Tx+b)=\sigma(w_{1}x_{1}+w_{2}x_{2}+\cdots+b)=\sigma(\theta^Tx)$
<ul>
<li>Sigmoid 函数： $s=\sigma(w^Tx+b)=\sigma(z)=\frac{1}{1+e^{-z}}$</li>
<li>如果 $z$ 的结果非常大，那么 $s$ 的结果接近于 $1$</li>
<li>如果 $z$ 的结果非常小，那么 $s$ 的结果接近于 $0$</li>
</ul>
</li>
</ul>
<blockquote>
<p>二分分类（Binary Classification）是一种常见的机器学习任务，主要用于将数据分为两个类别。在这种任务中，模型的目标是预测一个实例属于两个类别中的哪一个。</p>
</blockquote>
<h4 id="1212-逻辑回归损失函数">1.2.1.2 逻辑回归损失函数</h4>
<p><strong>损失函数（loss function）</strong> 用于衡量预测结果与真实值之间的误差。最简单的损失函数定义方式为平方差损失： $L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2$</p>
<blockquote>
<p>但是逻辑回归有自己的损失函数，这是因为上式存在多个局部最小点，不便于确定哪个点损失最小。</p>
</blockquote>
<p>逻辑回归一般使用</p>
<p>$$
L(\hat{y},y)=-(y\log \hat{y})-(1-y)\log(1-\hat{y})
$$</p>
<p>该式的理解：</p>
<ul>
<li>如果 $y=1$ ，损失为 $-\log \hat{y}$ ，那么要想损失越小， $\hat{y}$ 的值必须越大，即越趋近于或者等于 $1$</li>
<li>如果 $y=0$ ，损失为 $-\log (1-\hat{y})$ ，那么要想损失越小， $\hat{y}$ 的值必须越小，即越趋近于或者等于 $0$</li>
</ul>
<p>损失函数是在单个训练样本中定义的，它衡量了在单个训练样本上的表现，<strong>代价函数（cost function）</strong> 衡量的是在<strong>全体</strong>训练样本上的表现，即衡量参数 $w$ 和 $b$ 的效果，所有训练样本的损失平均值</p>
<p>$$
J(w,b)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})
$$</p>
<h3 id="122-梯度下降算法">1.2.2 梯度下降算法</h3>
<p>目的：使损失函数的值找到最小值</p>
<p>方式：梯度下降</p>
<p>函数的<strong>梯度（gradient）</strong> 指出了函数的最陡增长方向。<strong>按梯度的方向走，函数增长的就越快。那么按梯度的负方向走，函数值自然就降低得最快。</strong> 模型的训练目标即是寻找合适的 $w$ 和 $b$ 以最小化代价函数值。假设 $w$ 和 $b$ 都是一维实数，那么可以得到如下的 $J$ 关于 $w$ 和 $b$ 的图（根据损失函数得到）</p>
<p>可以看到，成本函数 $J$ 是一个凸函数，与非凸函数的区别在于其不含有多个局部最低。</p>
<p>参数 $w$ 和 $b$ 的更新公式为：</p>
<p>$w:=w-\alpha\frac{{dJ(w,b)}}{dw}$             $b:=b-\alpha\frac{{dJ(w,b)}}{db}$</p>
<blockquote>
<p>其中 $\alpha$ 表示学习速率，即每次更新的 $w$ 的步伐长度。当 $w$ 大于最优解 $w&rsquo;$ 时，导数大于 $0$ ，那么 $w$ 就会向更小的方向更新。反之就会向更大的方向更新。如此迭代直到收敛。</p>
</blockquote>
<p>通过平面理解梯度下降过程</p>
<div class="highlight"><pre tabindex="0" style="color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#99d1db">print</span>(<span style="color:#a6d189">&#39;hello&#39;</span>)
</span></span></code></pre></div>
    
  </div>

  

  
  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      <p>Powered by
    <a href="https://gohugo.io/">Hugo</a>
    and
    <a href="https://github.com/tomfran/typo">tomfran/typo</a>
</p>


<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>